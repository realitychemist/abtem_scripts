{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The basic function of this notebook is to take in a STEM image, run it through SingleOrigin, and then do (spatial) statistical analysis on it:\n",
    "1. Read in a STEM image\n",
    "2. Create a model of the structure in the image from a cif file\n",
    "3. Fit the atom columns using SingleOrigin\n",
    "4. Perform statistical analysis!\n",
    "\n",
    "First, let's take care of steps 1 through 3 all together:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import tifffile as tif\n",
    "import SingleOrigin as so\n",
    "reload(so)\n",
    "from copy import deepcopy\n",
    "from abtem_scripts import graphical, analysis_backend\n",
    "reload(graphical)\n",
    "reload(analysis_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_path = Path(r\"C:\\Users\\charles\\Documents\\AlScN\\img\\AlScN0.5.tif\")\n",
    "image_cropped = np.array(tif.imread(image_path))\n",
    "\n",
    "## Uncomment the following two lines if the image is not pre-cropped\n",
    "# image_cropped = qc.gui_crop(image_cropped)\n",
    "# image_cropped = so.image_norm(image_cropped)\n",
    "\n",
    "## Uncomment the following line to enable highpass filtering for the image (implemented in real space)\n",
    "# _, image_cropped = divide_image_frequencies(image_cropped, s=350, show_images=True)\n",
    "\n",
    "image_cropped = so.image_norm(image_cropped)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cif_path = graphical.gui_get_path()\n",
    "uc = so.UnitCell(cif_path, origin_shift=[0, 0, 0])\n",
    "uc.atoms.replace(\"Ti/O\", \"Ti/Zr\", inplace=True)\n",
    "\n",
    "za = [1, 1, 0]  # Zone axis direction\n",
    "a1 = [-1, 1, 0]  # Apparent horizontal axis in projection\n",
    "a2 = [0, 0, -1]  # Most vertical axis in projection\n",
    "\n",
    "# Ignore light elements for HAADF\n",
    "uc.project_zone_axis(za, a1, a2, ignore_elements=[\"N\"])\n",
    "uc.combine_prox_cols(toler=1e-2)\n",
    "\n",
    "# Uncomment the following line to check this output if changing the u.c.\n",
    "# uc.plot_unit_cell()\n",
    "\n",
    "hr_img = so.HRImage(image_cropped)\n",
    "lattice = hr_img.add_lattice(\"BZT\", uc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NOTE: There are a couple of steps in this cell that require interaction and will time out if ignored\n",
    "\n",
    "# If some FFT peaks are weak or absent (such as forbidden reflections),\n",
    "#  specify the order of the first peak that is clearly visible\n",
    "lattice.fft_get_basis_vect(a1_order=1, a2_order=1, sigma=2)\n",
    "\n",
    "# lattice.get_roi_mask_std(r=15, buffer=20, thresh=0.25, show_mask=True)  # Alternative to directly cropping the image in cell 2\n",
    "lattice.roi_mask = np.ones(image_cropped.shape)  # Use this line if the image was pre-cropped or cropped in cell 2\n",
    "\n",
    "lattice.define_reference_lattice()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lattice.fit_atom_columns(buffer=0, local_thresh_factor=0, use_background_param=True,\n",
    "                         use_bounds=True, use_circ_gauss=False, parallelize=True,\n",
    "                         peak_grouping_filter=None)\n",
    "\n",
    "# Must have only one column per projected unit cell.  If no sublattice meets this criteria,\n",
    "#  specify a specific column in the projected cell.\n",
    "lattice.refine_reference_lattice(filter_by='elem', sites_to_use='Ba')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lattice.get_fitting_residuals()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hr_img.plot_atom_column_positions(scatter_kwargs_dict={\"s\": 20}, scalebar_len_nm=None,\n",
    "                                  color_dict={\"Ba\": \"#FF0060\", \"Ti/Zr\": \"#84BABA\"},\n",
    "                                  outlier_disp_cutoff=100, fit_or_ref=\"ref\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hr_img.plot_disp_vects(sites_to_plot=[\"Ti/Zr\"], arrow_scale_factor=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to wrangle our data into a form that is going to be digestable (and sensical) to the statistical methods provided by pysal.  There is a *lot* going on here.  For the implementation details, check out the functions in `analysis_backend.py`.  Basically:\n",
    "1. We're making a copy of the the  `lattice.at_cols` DataFrame and immediately dropping some columns we won't need; this gives us a more convenient object to work with\n",
    "2. We need to throw out any (egregious) outliers: sometimes the fitting will leave one or two columns with crazy values for their intensity, and these will badly mess up our analysis, so we discard them\n",
    "3. We then need to normalize the column intensities, so they fall in the range 0-1.  There are a variety of different ways we can do this, but we need to do it somehow\n",
    "4. Then we want to discard columns which are irrelevant to our intensity statistics (i.e. columns corresponding to sites which have a different composition); if we compare, for example, Ti/Zr columns to Ba columns, we won't be able to learn anything meaningful about the Ti/Zr columns\n",
    "5. Step 5 messes up our indexing, so we use a nearest-neighbors search to re-index the remaining columns\n",
    "6. Next, we'll assign each column its neighborhood (as a mini adjacency list); this neighborhood is used for several things (adding boundary members to clusters when plotting, during the intensity normalization proccess for some methods, and when calculating dispersions from perfect sites),  but importantly it is *not* the same as the neighborhood that is used to calculate the actual statistics\n",
    "7. Finally, we will calculate the \"dispersion\" of fitted column positions from their perfect positions; these static shifts are our proxy for polarization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frame = deepcopy(lattice.at_cols)\n",
    "frame.drop([\"site_frac\", \"x\", \"y\", \"weight\"], axis=1, inplace=True)  # We don't need these cols\n",
    "frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "analysis_backend.reject_outliers(frame, mode=\"total_col_int\")\n",
    "analysis_backend.normalize_intensity(frame, lattice.a_2d, n=8, method=\"global_minmax\", kind=\"Ti/Zr\")\n",
    "analysis_backend.drop_elements(frame, [\"Ba\"])  # Doing this messes up the indexing in `neighborhood`\n",
    "# So we'll rebuild the neighborhood in the new indexing\n",
    "tree = analysis_backend.grow_tree(frame, lattice.a_2d, \"Ti/Zr\")\n",
    "frame[\"neighborhood\"] = frame.apply(lambda row: analysis_backend.get_near_neighbors(row, frame, lattice.a_2d, tree, n=8), axis=1)\n",
    "analysis_backend.disp_calc(frame, normalize=True, kind=\"Ti/Zr\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can actually do statistics:\n",
    "1. In the first cell, set the parameters for the analysis to be performed\n",
    "2. In the second cell, we'll run the analysis and print/plot the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kind: Literal[\"moran_global\", \"moran_local\", \"moran_global_bivariate\", \"moran_local_bivariate\",\n",
    "              \"geary_global\", \"geary_local\", \"geary_local_multivariate\"] = \"geary_local\"\n",
    "adj_type: Literal[\"rook\", \"queen\", \"king\", \"bishop\", \"120\"] = \"queen\"\n",
    "columns: list[str] = [\"dot_normal_disp\"]\n",
    "p: int = 10000\n",
    "printstats: bool = True\n",
    "sig: float = 0.05"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sts = analysis_backend.get_stats(df=frame, adj_type=adj_type, a2d=lattice.a_2d, kind=kind,\n",
    "                                 columns=columns, p=p, printstats=printstats)\n",
    "match kind:\n",
    "    case \"moran_global\" | \"moran_global_bivariate\":\n",
    "        analysis_backend.add_stats_to_frame(frame, sts, \"moran\")\n",
    "    case \"moran_local\" | \"moran_local_bivariate\":\n",
    "        analysis_backend.add_stats_to_frame(frame, sts, \"moran\")\n",
    "        analysis_backend.plot_moran_clusters(frame, image_cropped, sig=sig)\n",
    "    case \"geary_global\":\n",
    "        analysis_backend.add_stats_to_frame(frame, sts, \"geary\")\n",
    "    case \"geary_local\" | \"geary_local_multivariate\":\n",
    "        analysis_backend.add_stats_to_frame(frame, sts, \"geary\")\n",
    "        analysis_backend.plot_geary_clusters(frame, image_cropped, sig=sig)\n",
    "    case _:\n",
    "        raise ValueError"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
